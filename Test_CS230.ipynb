{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from scipy.io import loadmat\n",
    "import scipy.sparse as sparse\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding in .tfRecords from .Mat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_folders = [1821,2726, 1471, 1915]\n",
    "\n",
    "train_addrs = [[\"MatlabData/\"+str(j+1)+\"/trial_\"+str(i+1)+\".mat\" for i in range(len_folders[j])] for j in range(4)]\n",
    "train_addrs = list(itertools.chain.from_iterable(train_addrs))\n",
    "                   \n",
    "size_batch_in_memory = 1000\n",
    "num_batches = len(train_addrs)//size_batch_in_memory +((len(train_addrs)%size_batch_in_memory)>0)\n",
    "\n",
    "#Addresses to save the TFRecords file\n",
    "train_filename = [\"train_\"+str(i+1)+\".tfrecords\" for i in range(num_batches)]\n",
    "train_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_addrs[0:10], train_addrs[len_folders[0]:len_folders[0]+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def from_sparse_matrix_to_feature(features, mapping_number,name=\"default\"):\n",
    "    sparse_array = features[mapping_number]\n",
    "    index_x, index_y, values = sparse.find(sparse_array)\n",
    "    shape = sparse_array.shape\n",
    "    \n",
    "    feature = {    name+'_x': _int64_feature(index_x), \n",
    "                   name+'_y': _int64_feature(index_y),\n",
    "                   name+'_values': _float_feature(values),\n",
    "                   name+'_shape':  _int64_feature(shape)\n",
    "              }\n",
    "    return feature \n",
    "\n",
    "def from_int_matrix_to_feature(features, mapping_number, name=\"default\"):\n",
    "    return {name: _int64_feature(np.array(features[mapping_number].flatten()))}\n",
    "\n",
    "def from_float_matrix_to_feature(features, mapping_number, name=\"default\"):\n",
    "    return {name: _float_feature(np.array(features[mapping_number].flatten()))}\n",
    "\n",
    "def from_string_matrix_to_feature(features, mapping_number, name=\"default\"):\n",
    "    return {name: _bytes_feature(np.array(features[mapping_number].flatten(), dtype='str'))}\n",
    "\n",
    "def get_target_position(features, mapping_number=11, mapping_param_number=15, name=\"targetPos\"):\n",
    "    return {name: _float_feature(np.array(features[mapping_number][0,0][mapping_param_number]).flatten())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_name_to_type_and_position = np.load('mapping_Mat_Python.npy').item()\n",
    "sorted([(key,map_name_to_type_and_position[key][0]) for key in map_name_to_type_and_position.keys()],\n",
    "       key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create(filename_list, output_directory, \n",
    "           dev_size=500,test_size=500,shuffle=True, num_threads=1, max_size=1000):\n",
    "    \"\"\"\n",
    "    Randomly shuffles the data and divides the examples between train, dev and test sets. \n",
    "    \n",
    "    Args:\n",
    "        filename_list: list of the files containing the examples to convert into examples to include\n",
    "        output_directory: name of the directory where the .tfrecords will be saved\n",
    "        dev_size: dev test size\n",
    "        test_size: test set size\n",
    "        num_threads: NOT IMPLEMENTED\n",
    "        max_size: maximum size for one .tfrecords file (a \"shard\")\n",
    "    \"\"\"\n",
    "        \n",
    "    if shuffle:\n",
    "        random.shuffle(filename_list)\n",
    "        \n",
    "    train_size = len(filename_list)-(dev_size+test_size)\n",
    "    assert train_size > 0\n",
    "    \n",
    "    train_filenames = filename_list[:train_size]\n",
    "    dev_filenames   = filename_list[train_size:train_size+dev_size]\n",
    "    test_filenames  = filename_list[train_size+dev_size:]\n",
    "    \n",
    "    write_dataset(train_filenames, output_directory, num_threads, max_size, name='train')\n",
    "    print \"Train dataset completed\"\n",
    "    write_dataset(dev_filenames, output_directory, num_threads, max_size, name='dev')\n",
    "    print \"Dev dataset completed\"\n",
    "    write_dataset(test_filenames, output_directory, num_threads, max_size, name='test')\n",
    "    print \"Test dataset completed\"\n",
    "\n",
    "def write_dataset(filename_list, output_directory, num_threads=1, max_size=1000, name='train'):\n",
    "    \"\"\"\n",
    "    Writes all the examples in filename_list in several .tfrecords files.\n",
    "    \n",
    "    Args:\n",
    "        filename_list: list of the files containing the examples to convert into examples to include\n",
    "        output_directory: name of the directory where the .tfrecords will be saved\n",
    "        num_threads: NOT IMPLEMENTED\n",
    "        max_size: maximum size for one .tfrecords file (a \"shard\")\n",
    "        name : name of the .tfrecords file                 \n",
    "    \"\"\"\n",
    "        \n",
    "    dataset_size = len(filename_list)\n",
    "    \n",
    "    if (dataset_size%max_size):\n",
    "        num_shards   = dataset_size // max_size + 1\n",
    "    else:\n",
    "        num_shards = dataset_size // max_size\n",
    "        \n",
    "    print \"Number of shards for {} dataset: {}\".format(name,num_shards)\n",
    "    \n",
    "    for shard in range(num_shards):\n",
    "        # open the TFRecords file\n",
    "        output_filename = '%s-%.2d-of-%.2d.tfrecords' % (name, shard, num_shards)\n",
    "        output_file = os.path.join(output_directory, output_filename)\n",
    "        writer = tf.python_io.TFRecordWriter(output_file) \n",
    "        \n",
    "        # number of examples in this shard\n",
    "        if shard<(num_shards-1)|(not(dataset_size%max_size)):\n",
    "            shard_size = max_size\n",
    "        else:\n",
    "            shard_size = dataset_size - (num_shards-1)*max_size\n",
    "        \n",
    "        \n",
    "        for i in range(shard_size):\n",
    "            # print how many examples are saved every 100 images\n",
    "            if not i % 100:\n",
    "                print '{} Data: {}/{}'.format(name, shard*max_size+i+1, dataset_size)\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            # Load the the .mat file and convert it to a tf example\n",
    "            file_addrs = filename_list[max_size*shard+i]\n",
    "            example    = create_example_from_mat_file(file_addrs)\n",
    "            \n",
    "            # Serialize to string and write on the file\n",
    "            writer.write(example.SerializeToString())\n",
    "\n",
    "        writer.close() \n",
    "\n",
    "def create_example_from_mat_file(file_addrs):\n",
    "    \"\"\"\n",
    "    Converts a mat file containing one experiment into a tf Example\n",
    "    \n",
    "    Args:\n",
    "        file_addrs: address of the file containing the Matlab structure\n",
    "             \n",
    "    Returns:\n",
    "        tf Example reprensenting this experiment\n",
    "    \"\"\"\n",
    "    mat = loadmat(file_addrs)\n",
    "    features = list(mat['structarr'][0,0])\n",
    "\n",
    "    # Convert the features \n",
    "    feature = {}\n",
    "    for k in map_name_to_type_and_position:\n",
    "        map_number, map_type = map_name_to_type_and_position[k]\n",
    "        try:\n",
    "            if map_type == int:\n",
    "                feature.update(from_int_matrix_to_feature(features, map_number, name=k))\n",
    "            if map_type == float:\n",
    "                feature.update(from_float_matrix_to_feature(features, map_number, name=k))\n",
    "            if map_type == 'str':\n",
    "                feature.update(from_string_matrix_to_feature(features, map_number, name=k))\n",
    "            if map_type == 'sparse':\n",
    "                feature.update(from_sparse_matrix_to_feature(features, map_number, name=k))\n",
    "        except:\n",
    "            print k,file_addrs \n",
    "\n",
    "    feature.update(get_target_position(features))\n",
    "\n",
    "    # Create an example protocol buffer\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "create(train_addrs, 'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('mapping_Mat_Python.npy', map_name_to_type_and_position) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_positions_table = loadmat('posTarg.mat')['posTarg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len_sequence = 4782\n",
    "delay_time_min = 300 \n",
    "\n",
    "n_features_spikeRaster  = 96\n",
    "n_features_spikeRaster2 = 96\n",
    "n_total_features = n_features_spikeRaster + n_features_spikeRaster2\n",
    "num_classes = 48\n",
    "n_outputs = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "    \n",
    "    features = {\n",
    "                \"targetPos\" : tf.FixedLenFeature([3],tf.float32),\n",
    "                'numTarget' : tf.FixedLenFeature([], tf.int64),\n",
    "                'spikeRaster': tf.SparseFeature(index_key=['spikeRaster_x', 'spikeRaster_y'],\n",
    "                                                  value_key='spikeRaster_values',\n",
    "                                                  dtype=tf.float32, size=[n_features_spikeRaster, max_len_sequence]),\n",
    "                'spikeRaster2': tf.SparseFeature(index_key=['spikeRaster2_x', 'spikeRaster2_y'],\n",
    "                                                value_key='spikeRaster2_values',\n",
    "                                                dtype=tf.float32, size=[n_features_spikeRaster, max_len_sequence]),\n",
    "                \"spikeRaster_shape\": tf.FixedLenFeature([2],tf.int64),\n",
    "                \"isSuccessful\": tf.FixedLenFeature([1], tf.int64),\n",
    "                \"delayTime\": tf.FixedLenFeature([1], tf.float32),\n",
    "                'timeTargetOn': tf.FixedLenFeature([1], tf.float32)\n",
    "                \n",
    "               }\n",
    "    \n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    \n",
    "    # Predictive period => from timeTargetOn to delay_time_in\n",
    "    begin_time   = tf.cast(parsed_features[\"timeTargetOn\"], tf.int64)\n",
    "    begin_sparse = tf.pad(begin_time, [[1,0]], 'CONSTANT')\n",
    "    \n",
    "    # Preprocess spikeRaster => [Time Series n_steps x n_features_spikeRaster]\n",
    "    spikeRaster = tf.sparse_slice(parsed_features[\"spikeRaster\"],\n",
    "                                  begin_sparse,[n_features_spikeRaster,delay_time_min])\n",
    "    spikeRaster = tf.sparse_tensor_to_dense(spikeRaster)\n",
    "    spikeRaster = tf.transpose(spikeRaster)\n",
    "    spikeRaster.set_shape((delay_time_min, n_features_spikeRaster))\n",
    "\n",
    "    # Preprocess spikeRaster2 => [Time Series n_steps x n_features_spikeRaster]\n",
    "    spikeRaster2 = tf.sparse_slice(parsed_features[\"spikeRaster2\"],\n",
    "                                   begin_sparse,[n_features_spikeRaster2,delay_time_min])\n",
    "    spikeRaster2 = tf.sparse_tensor_to_dense(spikeRaster2)\n",
    "    spikeRaster2 = tf.transpose(spikeRaster2)\n",
    "    spikeRaster2.set_shape((delay_time_min, n_features_spikeRaster2))\n",
    "\n",
    "    # Combine spikeRaster + spikeRaster2\n",
    "    spikeRasters = tf.concat([spikeRaster,spikeRaster2], axis=1)\n",
    "\n",
    "    # isSuccessful into a boolean\n",
    "    isSuccessful = tf.cast(parsed_features[\"isSuccessful\"], tf.bool)\n",
    "    \n",
    "    # target Position\n",
    "    targetPos    = parsed_features['targetPos'][0:2]\n",
    "    # Preprocess target_pos\n",
    "    return spikeRasters, isSuccessful, targetPos, \\\n",
    "                         tf.cast(parsed_features['numTarget']-1, tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "size_batch = tf.placeholder(tf.int64, [])\n",
    "\n",
    "num_train_shards = 7\n",
    "num_dev_shards   = 1\n",
    "num_test_shards  = 1\n",
    "\n",
    "train_filenames = ['train-%.2d-of-%.2d.tfrecords' % (shard, num_train_shards) for shard in range(2)]\n",
    "dev_filenames   = ['dev-%.2d-of-%.2d.tfrecords' % (shard, num_dev_shards) for shard in range(num_dev_shards)]\n",
    "test_filenames  = ['test-%.2d-of-%.2d.tfrecords' % (shard, num_test_shards) for shard in range(num_test_shards)]\n",
    "\n",
    "data_directory  = 'Data'\n",
    "\n",
    "filenames = tf.placeholder(tf.string, [None])\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(_parse_function)\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=1000)\n",
    "dataset = dataset.batch(size_batch)\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "inputs, isSuccessful, target_pos, num_target = iterator.get_next()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_total = tf.data.TFRecordDataset(filenames)\n",
    "# dataset_total = dataset_total.map(_parse_function)\n",
    "\n",
    "# dataset_total = dataset_total.shuffle(buffer_size=10000)\n",
    "# dataset_total = dataset_total.batch(total_size)\n",
    "\n",
    "# iterator_total = dataset_total.make_initializable_iterator()\n",
    "\n",
    "# inputs_test, isSuccessful_test, target_pos_test, num_target_test = iterator_total.get_next()\n",
    "\n",
    "# # For testing over a dataset\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(iterator_total.initializer)\n",
    "#     X, isS, y_pos, y = sess.run([inputs_test, isSuccessful_test, target_pos_test, num_target_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_units = 128\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, [])\n",
    "\n",
    "# Build RNN cell\n",
    "encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "dropout_cell = tf.nn.rnn_cell.DropoutWrapper(encoder_cell, \n",
    "                              input_keep_prob=keep_prob, output_keep_prob=keep_prob)\n",
    "\n",
    "# Run Dynamic RNN\n",
    "# inputs: [batch_size, n_steps, n_inputs]\n",
    "rnn_outputs, _ = tf.nn.dynamic_rnn(\n",
    "     dropout_cell, inputs,\n",
    "    time_major=False,\n",
    "    dtype = tf.float32)\n",
    "\n",
    "# Recover meaningful outputs // Predict 3D positions\n",
    "rnn_outputs = rnn_outputs[:,-1,:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(rnn_outputs, n_outputs)\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=num_target, logits=logits))\n",
    "training_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "predicted_target = tf.cast(tf.argmax(logits, axis=1), tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predicted_position = tf.layers.dense(rnn_outputs, n_outputs)\n",
    "# loss = tf.losses.mean_squared_error(predictions=predicted_position, labels=target_pos)\n",
    "# training_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Tensor size [n_targets x 2]\n",
    "# target_positions = tf.placeholder(tf.float32, [num_classes,n_outputs])\n",
    "\n",
    "# Compute accuracy\n",
    "# dist_target = tf.reduce_sum((tf.expand_dims(predicted_position,1) - target_positions)**2,axis=2)\n",
    "# predicted_target = tf.cast(tf.argmin(dist_target, axis=1), tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_target, num_target), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', loss)\n",
    "accuracy_summary    = tf.summary.scalar('Accuracy', accuracy)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "#     saver.restore(sess, '/tmp/test_model_RNN_2')\n",
    "    for epoch in range(50):\n",
    "        sess.run(iterator.initializer, feed_dict={filenames:train_files, size_batch:128})\n",
    "        \n",
    "        current_loss, current_accuracy = sess.run([loss, accuracy],\n",
    "                                                  feed_dict={keep_prob:1.0}) \n",
    "#                                                   feed_dict={target_positions:target_positions_table})\n",
    "        print(\"Loss and accuracy after {} epochs: {} and {}\".format(epoch,\n",
    "                                                    current_loss, current_accuracy))         \n",
    "        while True:\n",
    "            try:\n",
    "                _= sess.run([training_op], feed_dict={keep_prob:0.7})\n",
    "#                                           feed_dict={target_positions:target_positions_table})        \n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "                \n",
    "    saver.save(sess, '/tmp/test_model_RNN_2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dev_files = [os.path.join(data_directory,data_file) for data_file in dev_filenames]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/test_model_RNN\")\n",
    "    sess.run(iterator.initializer, feed_dict={filenames:dev_files, size_batch:2000})\n",
    "    total_loss, total_accuracy = sess.run([loss, accuracy], feed_dict:{keep_prob:1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss, total_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 48\n",
    "lambda_l2 = 0.01\n",
    "\n",
    "inputs_svm = tf.reshape(inputs, [-1,delay_time_min*n_total_features])\n",
    "W = tf.Variable(np.zeros((delay_time_min*n_total_features,num_classes)), dtype=tf.float32)\n",
    "b = tf.Variable(np.zeros(num_classes), dtype=tf.float32)\n",
    "\n",
    "outputs_SVM = tf.matmul(inputs_svm,W)+ b\n",
    "y_SVM       = tf.one_hot(num_target, num_classes)\n",
    "\n",
    "hinge_loss = tf.losses.hinge_loss(y_SVM, outputs_SVM, reduction=tf.losses.Reduction.NONE)\n",
    "loss_SVM = tf.reduce_mean(tf.reduce_sum(hinge_loss,axis=1))\\\n",
    "                + lambda_l2*tf.reduce_sum(W**2)\n",
    "    \n",
    "\n",
    "### Kernel SVM\n",
    "\n",
    "# batch_size = 32\n",
    "# c = tf.Variable(tf.random_normal(shape=[1,batch_size]))\n",
    "\n",
    "\n",
    "# gamma = tf.constant(-10.0) # feel free to explore different values of gamma \n",
    "# dist = tf.reduce_sum(tf.square(inputs_svm), 1)\n",
    "# dist = tf.reshape(dist, [-1,1])\n",
    "# sq_dists = tf.add(tf.subtract(dist, tf.multiply(2., tf.matmul(inputs_svm,\n",
    "#                 tf.transpose(inputs_svm)))), tf.transpose(dist))\n",
    "# my_kernel = tf.exp(tf.multiply(gamma, tf.abs(sq_dists)))\n",
    "\n",
    "\n",
    "\n",
    "# model_output = tf.matmul(c, my_kernel)\n",
    "\n",
    "# first_term = tf.reduce_sum(c)\n",
    "\n",
    "# c_vec_cross = tf.matmul(tf.transpose(c), c)\n",
    "# y_target_cross = tf.matmul(y_SVM, tf.transpose(y_SVM))\n",
    "# second_term = tf.reduce_sum(tf.multiply(my_kernel,\n",
    "#                     tf.multiply(c_vec_cross, y_target_cross)))\n",
    "# loss_kernel_SVM = tf.negative(tf.subtract(first_term, second_term))\n",
    "\n",
    "training_op_SVM = tf.train.AdamOptimizer().minimize(loss_SVM)\n",
    "\n",
    "\n",
    "predicted_target = tf.cast(tf.argmax(outputs_SVM, axis=1), tf.int32)\n",
    "accuracy_SVM = tf.reduce_mean(tf.cast(tf.equal(predicted_target, num_target), tf.float32))\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir_SVM = \"tf_logs_SVM\"\n",
    "logdir_SVM = \"{}/run-{}/\".format(root_logdir_SVM, now)\n",
    "\n",
    "loss_summary_SVM        = tf.summary.scalar('Loss_SVM', loss_SVM)\n",
    "accuracy_summary_SVM    = tf.summary.scalar('Accuracy', accuracy_SVM)\n",
    "file_writer = tf.summary.FileWriter(logdir_SVM, tf.get_default_graph())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "train_files = [os.path.join(data_directory,data_file) for data_file in train_filenames]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        sess.run(iterator.initializer, feed_dict={filenames:train_files, size_batch:32})\n",
    "        current_loss, current_accuracy = sess.run([loss_SVM, accuracy_SVM])\n",
    "        print(\"Loss and accuracy after {} epochs: {} and {}\".format(epoch,\n",
    "                                                    current_loss, current_accuracy)) \n",
    "        while True:\n",
    "            try:\n",
    "                _, current_loss, current_accuracy = sess.run([training_op_SVM,loss_SVM, accuracy_SVM]) \n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "        \n",
    "    saver.save(sess, '/tmp/test_model_SVM')\n",
    "\n",
    "dev_files = [os.path.join(data_directory,data_file) for data_file in dev_filenames]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/test_model_SVM\")\n",
    "    sess.run(iterator.initializer, feed_dict={filenames:dev_files, size_batch:500})\n",
    "    total_loss, total_accuracy = sess.run([loss_SVM, accuracy_SVM])\n",
    "\n",
    "total_loss\n",
    "total_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    mat = loadmat(train_addrs[i])\n",
    "    features = list(mat['structarr'][0,0])\n",
    "    print (features[44])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted([(key,map_name_to_type_and_position[key][0]) for key in map_name_to_type_and_position.keys()],\n",
    "       key=lambda x:x[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
