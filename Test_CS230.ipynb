{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from scipy.io import loadmat\n",
    "import scipy.sparse as sparse\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding in .tfRecords from .Mat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_1.tfrecords',\n",
       " 'train_2.tfrecords',\n",
       " 'train_3.tfrecords',\n",
       " 'train_4.tfrecords',\n",
       " 'train_5.tfrecords',\n",
       " 'train_6.tfrecords',\n",
       " 'train_7.tfrecords',\n",
       " 'train_8.tfrecords']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_folders = [1821,2726, 1471, 1915]\n",
    "\n",
    "train_addrs = [[\"MatlabData/\"+str(j+1)+\"/trial_\"+str(i+1)+\".mat\" for i in range(len_folders[j])] for j in range(4)]\n",
    "train_addrs = list(itertools.chain.from_iterable(train_addrs))\n",
    "                   \n",
    "size_batch_in_memory = 1000\n",
    "num_batches = len(train_addrs)//size_batch_in_memory +((len(train_addrs)%size_batch_in_memory)>0)\n",
    "\n",
    "#Addresses to save the TFRecords file\n",
    "train_filename = [\"train_\"+str(i+1)+\".tfrecords\" for i in range(num_batches)]\n",
    "train_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['MatlabData/1/trial_1.mat',\n",
       "  'MatlabData/1/trial_2.mat',\n",
       "  'MatlabData/1/trial_3.mat',\n",
       "  'MatlabData/1/trial_4.mat',\n",
       "  'MatlabData/1/trial_5.mat',\n",
       "  'MatlabData/1/trial_6.mat',\n",
       "  'MatlabData/1/trial_7.mat',\n",
       "  'MatlabData/1/trial_8.mat',\n",
       "  'MatlabData/1/trial_9.mat',\n",
       "  'MatlabData/1/trial_10.mat'],\n",
       " ['MatlabData/2/trial_1.mat',\n",
       "  'MatlabData/2/trial_2.mat',\n",
       "  'MatlabData/2/trial_3.mat',\n",
       "  'MatlabData/2/trial_4.mat',\n",
       "  'MatlabData/2/trial_5.mat',\n",
       "  'MatlabData/2/trial_6.mat',\n",
       "  'MatlabData/2/trial_7.mat',\n",
       "  'MatlabData/2/trial_8.mat',\n",
       "  'MatlabData/2/trial_9.mat',\n",
       "  'MatlabData/2/trial_10.mat'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_addrs[0:10], train_addrs[len_folders[0]:len_folders[0]+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def from_sparse_matrix_to_feature(features, mapping_number,name=\"default\"):\n",
    "    sparse_array = features[mapping_number]\n",
    "    index_x, index_y, values = sparse.find(sparse_array)\n",
    "    shape = sparse_array.shape\n",
    "    \n",
    "    feature = {    name+'_x': _int64_feature(index_x), \n",
    "                   name+'_y': _int64_feature(index_y),\n",
    "                   name+'_values': _float_feature(values),\n",
    "                   name+'_shape':  _int64_feature(shape)\n",
    "              }\n",
    "    return feature \n",
    "\n",
    "def from_int_matrix_to_feature(features, mapping_number, name=\"default\"):\n",
    "    return {name: _int64_feature(np.array(features[mapping_number].flatten()))}\n",
    "\n",
    "def from_float_matrix_to_feature(features, mapping_number, name=\"default\"):\n",
    "    return {name: _float_feature(np.array(features[mapping_number].flatten()))}\n",
    "\n",
    "def from_string_matrix_to_feature(features, mapping_number, name=\"default\"):\n",
    "    return {name: _bytes_feature(np.array(features[mapping_number].flatten(), dtype='str'))}\n",
    "\n",
    "def get_target_position(features, mapping_number=11, mapping_param_number=15, name=\"targetPos\"):\n",
    "    return {name: _float_feature(np.array(features[mapping_number][0,0][mapping_param_number]).flatten())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('startCounter', 0),\n",
       " ('endCounter', 1),\n",
       " ('startDateNum', 2),\n",
       " ('startDateStr', 3),\n",
       " ('timeTargetOn', 4),\n",
       " ('timeTargetAcquire', 5),\n",
       " ('timeTargetHeld', 6),\n",
       " ('timeTrialEnd', 7),\n",
       " ('timeCueOn', 8),\n",
       " ('timeDelayBegins', 9),\n",
       " ('timeDelayFailed', 10),\n",
       " ('subject', 13),\n",
       " ('counter', 14),\n",
       " ('state', 15),\n",
       " ('numMarkers', 16),\n",
       " ('handPos', 17),\n",
       " ('eyePos', 18),\n",
       " ('decodePos', 19),\n",
       " ('decodeCommand', 20),\n",
       " ('decodeState', 21),\n",
       " ('decodeDiscrete', 22),\n",
       " ('cursorPos', 23),\n",
       " ('juice', 24),\n",
       " ('centerPos', 25),\n",
       " ('spikeRaster', 26),\n",
       " ('spikeRaster2', 27),\n",
       " ('numTotalSpikes', 28),\n",
       " ('numTotalSpikes2', 29),\n",
       " ('timeCerebusStart', 30),\n",
       " ('timeCerebusEnd', 31),\n",
       " ('timeCerebusStart2', 32),\n",
       " ('timeCerebusEnd2', 33),\n",
       " ('fakeSpikeRaster', 34),\n",
       " ('isSuccessful', 35),\n",
       " ('paramsValid', 36),\n",
       " ('cerebusOn', 37),\n",
       " ('allSignalsRecorded', 38),\n",
       " ('trialNum', 39),\n",
       " ('timeFirstTargetAcquire', 40),\n",
       " ('timeLastTargetAcquire', 41),\n",
       " ('trialLength', 42),\n",
       " ('delayTime', 43),\n",
       " ('numTarget', 44)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_name_to_type_and_position = np.load('mapping_Mat_Python.npy').item()\n",
    "sorted([(key,map_name_to_type_and_position[key][0]) for key in map_name_to_type_and_position.keys()],\n",
    "       key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create(filename_list, output_directory, \n",
    "           dev_size=500,test_size=500,shuffle=True, num_threads=1, max_size=1000):\n",
    "    \"\"\"\n",
    "    Randomly shuffles the data and divides the examples between train, dev and test sets. \n",
    "    \n",
    "    Args:\n",
    "        filename_list: list of the files containing the examples to convert into examples to include\n",
    "        output_directory: name of the directory where the .tfrecords will be saved\n",
    "        dev_size: dev test size\n",
    "        test_size: test set size\n",
    "        num_threads: NOT IMPLEMENTED\n",
    "        max_size: maximum size for one .tfrecords file (a \"shard\")\n",
    "    \"\"\"\n",
    "        \n",
    "    if shuffle:\n",
    "        random.shuffle(filename_list)\n",
    "        \n",
    "    train_size = len(filename_list)-(dev_size+test_size)\n",
    "    assert train_size > 0\n",
    "    \n",
    "    train_filenames = filename_list[:train_size]\n",
    "    dev_filenames   = filename_list[train_size:train_size+dev_size]\n",
    "    test_filenames  = filename_list[train_size+dev_size:]\n",
    "    \n",
    "    write_dataset(train_filenames, output_directory, num_threads, max_size, name='train')\n",
    "    print \"Train dataset completed\"\n",
    "    write_dataset(dev_filenames, output_directory, num_threads, max_size, name='dev')\n",
    "    print \"Dev dataset completed\"\n",
    "    write_dataset(test_filenames, output_directory, num_threads, max_size, name='test')\n",
    "    print \"Test dataset completed\"\n",
    "\n",
    "def write_dataset(filename_list, output_directory, num_threads=1, max_size=1000, name='train'):\n",
    "    \"\"\"\n",
    "    Writes all the examples in filename_list in several .tfrecords files.\n",
    "    \n",
    "    Args:\n",
    "        filename_list: list of the files containing the examples to convert into examples to include\n",
    "        output_directory: name of the directory where the .tfrecords will be saved\n",
    "        num_threads: NOT IMPLEMENTED\n",
    "        max_size: maximum size for one .tfrecords file (a \"shard\")\n",
    "        name : name of the .tfrecords file                 \n",
    "    \"\"\"\n",
    "        \n",
    "    dataset_size = len(filename_list)\n",
    "    \n",
    "    if (dataset_size%max_size):\n",
    "        num_shards   = dataset_size // max_size + 1\n",
    "    else:\n",
    "        num_shards = dataset_size // max_size\n",
    "        \n",
    "    print \"Number of shards for {} dataset: {}\".format(name,num_shards)\n",
    "    \n",
    "    for shard in range(num_shards):\n",
    "        # open the TFRecords file\n",
    "        output_filename = '%s-%.2d-of-%.2d.tfrecords' % (name, shard, num_shards)\n",
    "        output_file = os.path.join(output_directory, output_filename)\n",
    "        writer = tf.python_io.TFRecordWriter(output_file) \n",
    "        \n",
    "        # number of examples in this shard\n",
    "        if shard<(num_shards-1)|(not(dataset_size%max_size)):\n",
    "            shard_size = max_size\n",
    "        else:\n",
    "            shard_size = dataset_size - (num_shards-1)*max_size\n",
    "        \n",
    "        \n",
    "        for i in range(shard_size):\n",
    "            # print how many examples are saved every 100 images\n",
    "            if not i % 100:\n",
    "                print '{} Data: {}/{}'.format(name, shard*max_size+i+1, dataset_size)\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            # Load the the .mat file and convert it to a tf example\n",
    "            file_addrs = filename_list[max_size*shard+i]\n",
    "            example    = create_example_from_mat_file(file_addrs)\n",
    "            \n",
    "            # Serialize to string and write on the file\n",
    "            writer.write(example.SerializeToString())\n",
    "\n",
    "        writer.close() \n",
    "\n",
    "def create_example_from_mat_file(file_addrs):\n",
    "    \"\"\"\n",
    "    Converts a mat file containing one experiment into a tf Example\n",
    "    \n",
    "    Args:\n",
    "        file_addrs: address of the file containing the Matlab structure\n",
    "             \n",
    "    Returns:\n",
    "        tf Example reprensenting this experiment\n",
    "    \"\"\"\n",
    "    mat = loadmat(file_addrs)\n",
    "    features = list(mat['structarr'][0,0])\n",
    "\n",
    "    # Convert the features \n",
    "    feature = {}\n",
    "    for k in map_name_to_type_and_position:\n",
    "        map_number, map_type = map_name_to_type_and_position[k]\n",
    "        try:\n",
    "            if map_type == int:\n",
    "                feature.update(from_int_matrix_to_feature(features, map_number, name=k))\n",
    "            if map_type == float:\n",
    "                feature.update(from_float_matrix_to_feature(features, map_number, name=k))\n",
    "            if map_type == 'str':\n",
    "                feature.update(from_string_matrix_to_feature(features, map_number, name=k))\n",
    "            if map_type == 'sparse':\n",
    "                feature.update(from_sparse_matrix_to_feature(features, map_number, name=k))\n",
    "        except:\n",
    "            print k,file_addrs \n",
    "\n",
    "    feature.update(get_target_position(features))\n",
    "\n",
    "    # Create an example protocol buffer\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of shards for train dataset: 7\n",
      "train Data: 1/6933\n",
      "train Data: 101/6933\n",
      "train Data: 201/6933\n",
      "train Data: 301/6933\n",
      "train Data: 401/6933\n",
      "train Data: 501/6933\n",
      "train Data: 601/6933\n",
      "train Data: 701/6933\n",
      "timeCerebusEnd MatlabData/2/trial_1451.mat\n",
      "train Data: 801/6933\n",
      "train Data: 901/6933\n",
      "train Data: 1001/6933\n",
      "train Data: 1101/6933\n",
      "train Data: 1201/6933\n",
      "train Data: 1301/6933\n",
      "train Data: 1401/6933\n",
      "train Data: 1501/6933\n",
      "train Data: 1601/6933\n",
      "train Data: 1701/6933\n",
      "train Data: 1801/6933\n",
      "train Data: 1901/6933\n",
      "train Data: 2001/6933\n",
      "train Data: 2101/6933\n",
      "train Data: 2201/6933\n",
      "train Data: 2301/6933\n",
      "train Data: 2401/6933\n",
      "train Data: 2501/6933\n",
      "train Data: 2601/6933\n",
      "train Data: 2701/6933\n",
      "train Data: 2801/6933\n",
      "train Data: 2901/6933\n",
      "train Data: 3001/6933\n",
      "train Data: 3101/6933\n",
      "train Data: 3201/6933\n",
      "train Data: 3301/6933\n",
      "train Data: 3401/6933\n",
      "train Data: 3501/6933\n",
      "train Data: 3601/6933\n",
      "train Data: 3701/6933\n",
      "train Data: 3801/6933\n",
      "train Data: 3901/6933\n",
      "train Data: 4001/6933\n",
      "train Data: 4101/6933\n",
      "train Data: 4201/6933\n",
      "train Data: 4301/6933\n",
      "train Data: 4401/6933\n",
      "train Data: 4501/6933\n",
      "train Data: 4601/6933\n",
      "train Data: 4701/6933\n",
      "train Data: 4801/6933\n",
      "train Data: 4901/6933\n",
      "train Data: 5001/6933\n",
      "train Data: 5101/6933\n",
      "train Data: 5201/6933\n",
      "train Data: 5301/6933\n",
      "train Data: 5401/6933\n",
      "train Data: 5501/6933\n",
      "train Data: 5601/6933\n",
      "train Data: 5701/6933\n",
      "train Data: 5801/6933\n",
      "train Data: 5901/6933\n",
      "train Data: 6001/6933\n",
      "train Data: 6101/6933\n",
      "train Data: 6201/6933\n",
      "train Data: 6301/6933\n",
      "train Data: 6401/6933\n",
      "train Data: 6501/6933\n",
      "train Data: 6601/6933\n",
      "train Data: 6701/6933\n",
      "train Data: 6801/6933\n",
      "train Data: 6901/6933\n",
      "Train dataset completed\n",
      "Number of shards for dev dataset: 1\n",
      "dev Data: 1/500\n",
      "dev Data: 101/500\n",
      "timeCerebusEnd MatlabData/2/trial_1451.mat\n",
      "dev Data: 201/500\n",
      "dev Data: 301/500\n",
      "dev Data: 401/500\n",
      "Dev dataset completed\n",
      "Number of shards for test dataset: 1\n",
      "test Data: 1/500\n",
      "test Data: 101/500\n",
      "test Data: 201/500\n",
      "test Data: 301/500\n",
      "test Data: 401/500\n",
      "Test dataset completed\n"
     ]
    }
   ],
   "source": [
    "create(train_addrs, 'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('mapping_Mat_Python.npy', map_name_to_type_and_position) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_positions_table = loadmat('posTarg.mat')['posTarg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len_sequence = 4782\n",
    "delay_time_min = 300 \n",
    "\n",
    "n_features_spikeRaster  = 96\n",
    "n_features_spikeRaster2 = 96\n",
    "n_total_features = n_features_spikeRaster + n_features_spikeRaster2\n",
    "num_classes = 48\n",
    "n_outputs = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "    \n",
    "    features = {\n",
    "                \"targetPos\" : tf.FixedLenFeature([3],tf.float32),\n",
    "                'numTarget' : tf.FixedLenFeature([], tf.int64),\n",
    "                'spikeRaster': tf.SparseFeature(index_key=['spikeRaster_x', 'spikeRaster_y'],\n",
    "                                                  value_key='spikeRaster_values',\n",
    "                                                  dtype=tf.float32, size=[n_features_spikeRaster, max_len_sequence]),\n",
    "                'spikeRaster2': tf.SparseFeature(index_key=['spikeRaster2_x', 'spikeRaster2_y'],\n",
    "                                                value_key='spikeRaster2_values',\n",
    "                                                dtype=tf.float32, size=[n_features_spikeRaster, max_len_sequence]),\n",
    "                \"spikeRaster_shape\": tf.FixedLenFeature([2],tf.int64),\n",
    "                \"isSuccessful\": tf.FixedLenFeature([1], tf.int64),\n",
    "                \"delayTime\": tf.FixedLenFeature([1], tf.float32),\n",
    "                'timeTargetOn': tf.FixedLenFeature([1], tf.float32)\n",
    "                \n",
    "               }\n",
    "    \n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    \n",
    "    # Predictive period => from timeTargetOn to delay_time_in\n",
    "    begin_time   = tf.cast(parsed_features[\"timeTargetOn\"], tf.int64)\n",
    "    begin_sparse = tf.pad(begin_time, [[1,0]], 'CONSTANT')\n",
    "    \n",
    "    # Preprocess spikeRaster => [Time Series n_steps x n_features_spikeRaster]\n",
    "    spikeRaster = tf.sparse_slice(parsed_features[\"spikeRaster\"],\n",
    "                                  begin_sparse,[n_features_spikeRaster,delay_time_min])\n",
    "    spikeRaster = tf.sparse_tensor_to_dense(spikeRaster)\n",
    "    spikeRaster = tf.transpose(spikeRaster)\n",
    "    spikeRaster.set_shape((delay_time_min, n_features_spikeRaster))\n",
    "\n",
    "    # Preprocess spikeRaster2 => [Time Series n_steps x n_features_spikeRaster]\n",
    "    spikeRaster2 = tf.sparse_slice(parsed_features[\"spikeRaster2\"],\n",
    "                                   begin_sparse,[n_features_spikeRaster2,delay_time_min])\n",
    "    spikeRaster2 = tf.sparse_tensor_to_dense(spikeRaster2)\n",
    "    spikeRaster2 = tf.transpose(spikeRaster2)\n",
    "    spikeRaster2.set_shape((delay_time_min, n_features_spikeRaster2))\n",
    "\n",
    "    # Combine spikeRaster + spikeRaster2\n",
    "    spikeRasters = tf.concat([spikeRaster,spikeRaster2], axis=1)\n",
    "\n",
    "    # isSuccessful into a boolean\n",
    "    isSuccessful = tf.cast(parsed_features[\"isSuccessful\"], tf.bool)\n",
    "    \n",
    "    # target Position\n",
    "    targetPos    = parsed_features['targetPos'][0:2]\n",
    "    # Preprocess target_pos\n",
    "    return spikeRasters, isSuccessful, targetPos, \\\n",
    "                         tf.cast(parsed_features['numTarget']-1, tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SparseFeature is a complicated feature config and should only be used after careful consideration of VarLenFeature.\n",
      "WARNING:tensorflow:SparseFeature is a complicated feature config and should only be used after careful consideration of VarLenFeature.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "size_batch = tf.placeholder(tf.int64, [])\n",
    "\n",
    "num_train_shards = 7\n",
    "num_dev_shards   = 1\n",
    "num_test_shards  = 1\n",
    "\n",
    "train_filenames = ['train-%.2d-of-%.2d.tfrecords' % (shard, num_train_shards) for shard in range(2)]\n",
    "dev_filenames   = ['dev-%.2d-of-%.2d.tfrecords' % (shard, num_dev_shards) for shard in range(num_dev_shards)]\n",
    "test_filenames  = ['test-%.2d-of-%.2d.tfrecords' % (shard, num_test_shards) for shard in range(num_test_shards)]\n",
    "\n",
    "data_directory  = 'Data'\n",
    "\n",
    "filenames = tf.placeholder(tf.string, [None])\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(_parse_function)\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=1000)\n",
    "dataset = dataset.batch(size_batch)\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "inputs, isSuccessful, target_pos, num_target = iterator.get_next()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_total = tf.data.TFRecordDataset(filenames)\n",
    "# dataset_total = dataset_total.map(_parse_function)\n",
    "\n",
    "# dataset_total = dataset_total.shuffle(buffer_size=10000)\n",
    "# dataset_total = dataset_total.batch(total_size)\n",
    "\n",
    "# iterator_total = dataset_total.make_initializable_iterator()\n",
    "\n",
    "# inputs_test, isSuccessful_test, target_pos_test, num_target_test = iterator_total.get_next()\n",
    "\n",
    "# # For testing over a dataset\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(iterator_total.initializer)\n",
    "#     X, isS, y_pos, y = sess.run([inputs_test, isSuccessful_test, target_pos_test, num_target_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM Classifier\n",
    "num_classes = 48\n",
    "lambda_l2 = 0.01\n",
    "\n",
    "inputs_svm = tf.reshape(inputs, [-1,delay_time_min*n_total_features])\n",
    "W = tf.Variable(np.zeros((delay_time_min*n_total_features,num_classes)), dtype=tf.float32)\n",
    "b = tf.Variable(np.zeros(num_classes), dtype=tf.float32)\n",
    "\n",
    "outputs_SVM = tf.matmul(inputs_svm,W)+ b\n",
    "y_SVM       = tf.one_hot(num_target, num_classes)\n",
    "\n",
    "hinge_loss = tf.losses.hinge_loss(y_SVM, outputs_SVM, reduction=tf.losses.Reduction.NONE)\n",
    "loss_SVM = tf.reduce_mean(tf.reduce_sum(hinge_loss,axis=1))\\\n",
    "                + lambda_l2*tf.reduce_sum(W**2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# c = tf.Variable(tf.random_normal(shape=[1,batch_size]))\n",
    "\n",
    "\n",
    "# gamma = tf.constant(-10.0) # feel free to explore different values of gamma \n",
    "# dist = tf.reduce_sum(tf.square(inputs_svm), 1)\n",
    "# dist = tf.reshape(dist, [-1,1])\n",
    "# sq_dists = tf.add(tf.subtract(dist, tf.multiply(2., tf.matmul(inputs_svm,\n",
    "#                 tf.transpose(inputs_svm)))), tf.transpose(dist))\n",
    "# my_kernel = tf.exp(tf.multiply(gamma, tf.abs(sq_dists)))\n",
    "\n",
    "\n",
    "\n",
    "# model_output = tf.matmul(c, my_kernel)\n",
    "\n",
    "# first_term = tf.reduce_sum(c)\n",
    "\n",
    "# c_vec_cross = tf.matmul(tf.transpose(c), c)\n",
    "# y_target_cross = tf.matmul(y_SVM, tf.transpose(y_SVM))\n",
    "# second_term = tf.reduce_sum(tf.multiply(my_kernel,\n",
    "#                     tf.multiply(c_vec_cross, y_target_cross)))\n",
    "# loss_kernel_SVM = tf.negative(tf.subtract(first_term, second_term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_op_SVM = tf.train.AdamOptimizer().minimize(loss_SVM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_target = tf.cast(tf.argmax(outputs_SVM, axis=1), tf.int32)\n",
    "accuracy_SVM = tf.reduce_mean(tf.cast(tf.equal(predicted_target, num_target), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir_SVM = \"tf_logs_SVM\"\n",
    "logdir_SVM = \"{}/run-{}/\".format(root_logdir_SVM, now)\n",
    "\n",
    "loss_summary_SVM        = tf.summary.scalar('Loss_SVM', loss_SVM)\n",
    "accuracy_summary_SVM    = tf.summary.scalar('Accuracy', accuracy_SVM)\n",
    "file_writer = tf.summary.FileWriter(logdir_SVM, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and accuracy after 0 epochs: 48.0 and 0.0\n",
      "Loss and accuracy after 1 epochs: 1.18297934532 and 0.96875\n",
      "Loss and accuracy after 2 epochs: 0.833225786686 and 1.0\n",
      "Loss and accuracy after 3 epochs: 0.900442063808 and 0.9375\n",
      "Loss and accuracy after 4 epochs: 1.1018345356 and 0.9375\n",
      "Loss and accuracy after 5 epochs: 1.01617598534 and 1.0\n",
      "Loss and accuracy after 6 epochs: 0.917141497135 and 0.96875\n",
      "Loss and accuracy after 7 epochs: 1.03163313866 and 1.0\n",
      "Loss and accuracy after 8 epochs: 1.17371237278 and 0.9375\n",
      "Loss and accuracy after 9 epochs: 1.35833728313 and 0.90625\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "train_files = [os.path.join(data_directory,data_file) for data_file in train_filenames]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        sess.run(iterator.initializer, feed_dict={filenames:train_files, size_batch:32})\n",
    "        current_loss, current_accuracy = sess.run([loss_SVM, accuracy_SVM])\n",
    "        print(\"Loss and accuracy after {} epochs: {} and {}\".format(epoch,\n",
    "                                                    current_loss, current_accuracy)) \n",
    "        while True:\n",
    "            try:\n",
    "                _, current_loss, current_accuracy = sess.run([training_op_SVM,loss_SVM, accuracy_SVM]) \n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "        \n",
    "    saver.save(sess, '/tmp/test_model_SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/test_model_SVM\n"
     ]
    }
   ],
   "source": [
    "dev_files = [os.path.join(data_directory,data_file) for data_file in dev_filenames]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/test_model_SVM\")\n",
    "    sess.run(iterator.initializer, feed_dict={filenames:dev_files, size_batch:500})\n",
    "    total_loss, total_accuracy = sess.run([loss_SVM, accuracy_SVM])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.075999998"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss\n",
    "total_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_units = 128\n",
    "\n",
    "# Build RNN cell\n",
    "encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "\n",
    "# Run Dynamic RNN\n",
    "# inputs: [batch_size, n_steps, n_inputs]\n",
    "rnn_outputs, _ = tf.nn.dynamic_rnn(\n",
    "    encoder_cell, inputs,\n",
    "    time_major=False,\n",
    "    dtype = tf.float32)\n",
    "\n",
    "# Recover meaningful outputs // Predict 3D positions\n",
    "rnn_outputs = rnn_outputs[:,-1,:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(rnn_outputs, n_outputs)\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=num_target, logits=logits))\n",
    "training_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "predicted_target = tf.cast(tf.argmax(logits, axis=1), tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predicted_position = tf.layers.dense(rnn_outputs, n_outputs)\n",
    "# loss = tf.losses.mean_squared_error(predictions=predicted_position, labels=target_pos)\n",
    "# training_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Tensor size [n_targets x 2]\n",
    "target_positions = tf.placeholder(tf.float32, [num_classes,n_outputs])\n",
    "\n",
    "# Compute accuracy\n",
    "# dist_target = tf.reduce_sum((tf.expand_dims(predicted_position,1) - target_positions)**2,axis=2)\n",
    "# predicted_target = tf.cast(tf.argmin(dist_target, axis=1), tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_target, num_target), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', loss)\n",
    "accuracy_summary    = tf.summary.scalar('Accuracy', accuracy)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/test_model_RNN\n",
      "Loss and accuracy after 0 epochs: 0.819498896599 and 0.8203125\n",
      "Loss and accuracy after 1 epochs: 0.677690505981 and 0.84375\n",
      "Loss and accuracy after 2 epochs: 0.669375777245 and 0.828125\n",
      "Loss and accuracy after 3 epochs: 0.702750623226 and 0.7890625\n",
      "Loss and accuracy after 4 epochs: 0.582062959671 and 0.8984375\n",
      "Loss and accuracy after 5 epochs: 0.590732812881 and 0.8984375\n",
      "Loss and accuracy after 6 epochs: 0.578971982002 and 0.8671875\n",
      "Loss and accuracy after 7 epochs: 0.569022297859 and 0.875\n",
      "Loss and accuracy after 8 epochs: 0.482404530048 and 0.890625\n",
      "Loss and accuracy after 9 epochs: 0.452738434076 and 0.9296875\n",
      "Loss and accuracy after 10 epochs: 0.556293129921 and 0.890625\n",
      "Loss and accuracy after 11 epochs: 0.543324410915 and 0.875\n",
      "Loss and accuracy after 12 epochs: 0.551054179668 and 0.8828125\n",
      "Loss and accuracy after 13 epochs: 0.463853627443 and 0.9296875\n",
      "Loss and accuracy after 14 epochs: 0.475617229939 and 0.90625\n",
      "Loss and accuracy after 15 epochs: 0.541057944298 and 0.90625\n",
      "Loss and accuracy after 16 epochs: 0.347050011158 and 0.9453125\n",
      "Loss and accuracy after 17 epochs: 0.516327619553 and 0.875\n",
      "Loss and accuracy after 18 epochs: 0.370939135551 and 0.921875\n",
      "Loss and accuracy after 19 epochs: 0.43302243948 and 0.9296875\n",
      "Loss and accuracy after 20 epochs: 0.301539182663 and 0.9453125\n",
      "Loss and accuracy after 21 epochs: 0.401403367519 and 0.953125\n",
      "Loss and accuracy after 22 epochs: 0.695755720139 and 0.84375\n",
      "Loss and accuracy after 23 epochs: 0.604999423027 and 0.859375\n",
      "Loss and accuracy after 24 epochs: 0.38338547945 and 0.9609375\n",
      "Loss and accuracy after 25 epochs: 0.28375056386 and 0.953125\n",
      "Loss and accuracy after 26 epochs: 0.313826560974 and 0.9453125\n",
      "Loss and accuracy after 27 epochs: 0.293536692858 and 0.9609375\n",
      "Loss and accuracy after 28 epochs: 0.278235256672 and 0.96875\n",
      "Loss and accuracy after 29 epochs: 0.269078850746 and 0.9765625\n",
      "Loss and accuracy after 30 epochs: 0.186076670885 and 0.984375\n",
      "Loss and accuracy after 31 epochs: 0.151589393616 and 0.9921875\n",
      "Loss and accuracy after 32 epochs: 0.197309330106 and 0.9765625\n",
      "Loss and accuracy after 33 epochs: 0.179016977549 and 0.9765625\n",
      "Loss and accuracy after 34 epochs: 0.170668244362 and 0.984375\n",
      "Loss and accuracy after 35 epochs: 0.286425292492 and 0.9765625\n",
      "Loss and accuracy after 36 epochs: 0.355130314827 and 0.9609375\n",
      "Loss and accuracy after 37 epochs: 0.293648600578 and 0.953125\n",
      "Loss and accuracy after 38 epochs: 0.264797151089 and 0.9765625\n",
      "Loss and accuracy after 39 epochs: 0.136095404625 and 0.9921875\n",
      "Loss and accuracy after 40 epochs: 0.104354739189 and 1.0\n",
      "Loss and accuracy after 41 epochs: 0.0979122668505 and 1.0\n",
      "Loss and accuracy after 42 epochs: 0.0915334373713 and 1.0\n",
      "Loss and accuracy after 43 epochs: 0.0950333327055 and 0.9921875\n",
      "Loss and accuracy after 44 epochs: 0.0829179435968 and 1.0\n",
      "Loss and accuracy after 45 epochs: 0.0748299360275 and 1.0\n",
      "Loss and accuracy after 46 epochs: 0.0757308229804 and 1.0\n",
      "Loss and accuracy after 47 epochs: 0.0856319516897 and 0.9921875\n",
      "Loss and accuracy after 48 epochs: 0.0529927536845 and 1.0\n",
      "Loss and accuracy after 49 epochs: 0.0559418946505 and 1.0\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "    saver.restore(sess, '/tmp/test_model_RNN')\n",
    "    for epoch in range(50):\n",
    "        sess.run(iterator.initializer, feed_dict={filenames:train_files, size_batch:128})\n",
    "        \n",
    "        current_loss, current_accuracy = sess.run([loss, accuracy]) \n",
    "#                                                   feed_dict={target_positions:target_positions_table})\n",
    "        print(\"Loss and accuracy after {} epochs: {} and {}\".format(epoch,\n",
    "                                                    current_loss, current_accuracy))         \n",
    "        while True:\n",
    "            try:\n",
    "                _= sess.run([training_op])#, feed_dict={target_positions:target_positions_table})        \n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "                \n",
    "    saver.save(sess, '/tmp/test_model_RNN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/test_model_RNN\n"
     ]
    }
   ],
   "source": [
    "dev_files = [os.path.join(data_directory,data_file) for data_file in train_filenames]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/test_model_RNN\")\n",
    "    sess.run(iterator.initializer, feed_dict={filenames:dev_files, size_batch:2000})\n",
    "    total_loss, total_accuracy = sess.run([loss, accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.067291208, 0.99599999)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss, total_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    mat = loadmat(train_addrs[i])\n",
    "    features = list(mat['structarr'][0,0])\n",
    "    print (features[44])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted([(key,map_name_to_type_and_position[key][0]) for key in map_name_to_type_and_position.keys()],\n",
    "       key=lambda x:x[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
