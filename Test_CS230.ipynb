{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from scipy.io import loadmat\n",
    "import scipy.sparse as sparse\n",
    "import numpy as np\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding in .tfRecords from .Mat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_addrs = [\"MatlabData/trial_\"+str(i+1)+\".mat\" for i in range(7361) ]\n",
    "train_addrs[0:5]\n",
    "\n",
    "size_batch_in_memory = 1000\n",
    "num_batches = len(train_addrs)//size_batch +((len(train_addrs)%size_batch)>0)\n",
    "\n",
    "#Addresses to save the TFRecords file\n",
    "train_filename = [\"train_\"+str(i+1)+\".tfrecords\" for i in range(num_batches)]\n",
    "train_filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def from_sparse_matrix_to_feature(features, mapping_number,name=\"default\"):\n",
    "    sparse_array = features[mapping_number]\n",
    "    index_x, index_y, values = sparse.find(sparse_array)\n",
    "    shape = sparse_array.shape\n",
    "    \n",
    "    feature = {    name+'_x': _int64_feature(index_x), \n",
    "                   name+'_y': _int64_feature(index_y),\n",
    "                   name+'_values': _float_feature(values),\n",
    "                   name+'_shape':  _int64_feature(shape)\n",
    "              }\n",
    "    return feature \n",
    "\n",
    "def from_int_matrix_to_feature(features, mapping_number, name=\"default\"):\n",
    "    return {name: _int64_feature(np.array(features[mapping_number].flatten()))}\n",
    "\n",
    "def from_float_matrix_to_feature(features, mapping_number, name=\"default\"):\n",
    "    return {name: _float_feature(np.array(features[mapping_number].flatten()))}\n",
    "\n",
    "def from_string_matrix_to_feature(features, mapping_number, name=\"default\"):\n",
    "    return {name: _bytes_feature(np.array(features[mapping_number].flatten(), dtype='str'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('mapping_Mat_Python.npy', map_name_to_type_and_position) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_name_to_type_and_position = np.load('mapping_Mat_Python.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate on the number of files\n",
    "for j in range(num_batches):\n",
    "    # open the TFRecords file\n",
    "    writer = tf.python_io.TFRecordWriter(train_filename[j]) \n",
    "    \n",
    "    for i in range(size_batch_in_memory):\n",
    "        \n",
    "        #Case where no file left\n",
    "        if j*size_batch_in_memory+i>=len(train_addrs):\n",
    "            writer.close()\n",
    "            break\n",
    "            \n",
    "        # print how many examples are saved every 100 images\n",
    "        if not i % 100:\n",
    "            print 'Train data: {}/{}'.format(j*size_batch+i, len(train_addrs))\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        # Load the the .mat file \n",
    "        mat = loadmat(train_addrs[j*size_batch+i])\n",
    "        features = list(mat['structarr'][0,0])\n",
    "        \n",
    "        # Convert the features \n",
    "        feature = {}\n",
    "        for k in map_name_to_type_and_position:\n",
    "            map_number, map_type = map_name_to_type_and_position[k]\n",
    "            if map_type == int:\n",
    "                feature.update(from_int_matrix_to_feature(features, map_number, name=k))\n",
    "            if map_type == float:\n",
    "                feature.update(from_float_matrix_to_feature(features, map_number, name=k))\n",
    "            if map_type == 'str':\n",
    "                feature.update(from_string_matrix_to_feature(features, map_number, name=k))\n",
    "            if map_type == 'sparse':\n",
    "                feature.update(from_sparse_matrix_to_feature(features, map_number, name=k))\n",
    "                \n",
    "\n",
    "        # Create an example protocol buffer\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "        # Serialize to string and write on the file\n",
    "        writer.write(example.SerializeToString())\n",
    "\n",
    "    writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len_sequence = 2568\n",
    "\n",
    "n_steps = 200\n",
    "n_steps_to_predict = 10\n",
    "n_features_spikeRaster = 96\n",
    "n_dims_output = 3\n",
    "\n",
    "n_inputs = n_features_spikeRaster + n_dims_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "    \n",
    "    features = {\n",
    "                \"handPos\" : tf.VarLenFeature(tf.float32),\n",
    "                'spikeRaster': tf.SparseFeature(index_key=['spikeRaster_x', 'spikeRaster_y'],\n",
    "                                                  value_key='spikeRaster_values',\n",
    "                                                  dtype=tf.float32, size=[n_features_spikeRaster, max_len_sequence]),\n",
    "                \"spikeRaster_shape\": tf.FixedLenFeature([2],tf.int64)               \n",
    "               }\n",
    "    \n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    \n",
    "    # Preprocess spikeRaster => [Time Series n_steps x n_features_spikeRaster]\n",
    "    parsed_features[\"spikeRaster\"] = tf.sparse_slice(parsed_features[\"spikeRaster\"],\n",
    "                                                     [0,0],parsed_features[\"spikeRaster_shape\"])\n",
    "    parsed_features[\"spikeRaster\"] = tf.sparse_tensor_to_dense(parsed_features[\"spikeRaster\"])\n",
    "    spikeRaster = tf.reshape(tf.transpose(parsed_features[\"spikeRaster\"]), [-1,n_features_spikeRaster])\n",
    "    \n",
    "    # Preprocess lengths of sequences = []\n",
    "    seq_length = tf.cast(parsed_features[\"spikeRaster_shape\"][1], tf.int32)\n",
    "        \n",
    "    # Preprocess handPos = [n_steps x 3] => HELPER\n",
    "    handPos = tf.sparse_tensor_to_dense(parsed_features[\"handPos\"])\n",
    "    handPos = tf.transpose(tf.reshape(handPos, [n_dims_output,-1]))\n",
    "    \n",
    "    # Useful features\n",
    "    features = tf.concat([spikeRaster,handPos], axis=1)    \n",
    "    return features\n",
    "\n",
    "def get_slices(x):\n",
    "    num_slices = tf.shape(x, out_type=tf.int64)[0] - n_steps - n_steps_to_predict + 1\n",
    "    return tf.data.Dataset.range(num_slices).map(lambda i: (x[i:i + n_steps] , \n",
    "                                                            x[i+n_steps:i+n_steps+n_steps_to_predict,-3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SparseFeature is a complicated feature config and should only be used after careful consideration of VarLenFeature.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "size_batch = 32\n",
    "filenames = [\"Data/train_\"+str(i+1)+\".tfrecords\" for i in range(1)]\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(_parse_function)\n",
    "dataset = dataset.flat_map(get_slices)\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=10000)\n",
    "dataset = dataset.batch(size_batch)\n",
    "dataset = dataset.repeat(num_epochs)\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "inputs, real_position = iterator.get_next()\n",
    "\n",
    "inputs.set_shape([None,n_steps,n_inputs])\n",
    "real_position.set_shape([None,n_steps_to_predict,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    e1, e2 = sess.run([inputs, real_position])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'IteratorGetNext:0' shape=(?, 200, 99) dtype=float32>,\n",
       " <tf.Tensor 'IteratorGetNext:1' shape=(?, 10, 3) dtype=float32>)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, real_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "is_training = tf.placeholder(tf.bool, [])\n",
    "\n",
    "keep_prob = 0.5\n",
    "num_units = 128\n",
    "\n",
    "# Build RNN cell\n",
    "encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "\n",
    "# Run Dynamic RNN\n",
    "# inputs: [batch_size, n_steps, n_inputs]\n",
    "rnn_outputs, _ = tf.nn.dynamic_rnn(\n",
    "    encoder_cell, inputs,\n",
    "    time_major=False,\n",
    "    dtype = tf.float32)\n",
    "\n",
    "# Recover meaningful outputs // Predict 3D positions\n",
    "rnn_outputs = rnn_outputs[:,- n_steps_to_predict:,:]\n",
    "predicted_position = tf.layers.dense(rnn_outputs, 3)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(predictions=predicted_position, labels=real_position)\n",
    "training_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', loss)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 epochs: 69063.28125\n",
      "Loss after 100 iteration: 34369.8398438\n",
      "Loss after 200 iteration: 17333.1113281\n",
      "Loss after 300 iteration: 1036.01477051\n",
      "Loss after 400 iteration: 560.381408691\n",
      "Loss after 500 iteration: 48975.703125\n",
      "Loss after 600 iteration: 340.898895264\n",
      "Loss after 700 iteration: 16252.4794922\n",
      "Loss after 800 iteration: 587.118469238\n",
      "Loss after 900 iteration: 15884.0898438\n",
      "Loss after 1000 iteration: 384.318023682\n",
      "Loss after 1100 iteration: 79.0458145142\n",
      "Loss after 1200 iteration: 184.307434082\n",
      "Loss after 1300 iteration: 38.5350723267\n",
      "Loss after 1400 iteration: 92.8044204712\n",
      "Loss after 1500 iteration: 17.2639293671\n",
      "Loss after 1600 iteration: 198.546676636\n",
      "Loss after 1700 iteration: 155.789108276\n",
      "Loss after 1800 iteration: 83.1469726562\n",
      "Loss after 1900 iteration: 64.4686889648\n",
      "Loss after 2000 iteration: 147.254638672\n",
      "Loss after 2100 iteration: 94.3248214722\n",
      "Loss after 2200 iteration: 53.9316978455\n",
      "Loss after 2300 iteration: 36.1063995361\n",
      "Loss after 2400 iteration: 62.4742736816\n",
      "Loss after 2500 iteration: 32.0783119202\n",
      "Loss after 2600 iteration: 45.0978507996\n",
      "Loss after 2700 iteration: 13.1010971069\n",
      "Loss after 2800 iteration: 26.3091220856\n",
      "Loss after 2900 iteration: 0.473439127207\n",
      "Loss after 3000 iteration: 6.51426076889\n",
      "Loss after 3100 iteration: 18.5561618805\n",
      "Loss after 3200 iteration: 8.6356420517\n",
      "Loss after 3300 iteration: 3.59766387939\n",
      "Loss after 3400 iteration: 4.11965847015\n",
      "Loss after 3500 iteration: 19.4543857574\n",
      "Loss after 3600 iteration: 682.368469238\n",
      "Loss after 3700 iteration: 145.245773315\n",
      "Loss after 3800 iteration: 109.519416809\n",
      "Loss after 3900 iteration: 41.3228607178\n",
      "Loss after 4000 iteration: 35.719997406\n",
      "Loss after 4100 iteration: 7.46215105057\n",
      "Loss after 4200 iteration: 5.41359853745\n",
      "Loss after 4300 iteration: 3.00380992889\n",
      "Loss after 4400 iteration: 3.03808283806\n",
      "Loss after 4500 iteration: 12.6081151962\n",
      "Loss after 4600 iteration: 21.9193172455\n",
      "Loss after 4700 iteration: 53.7172546387\n",
      "Loss after 4800 iteration: 19.9703960419\n",
      "Loss after 4900 iteration: 0.868059456348\n",
      "Loss after 5000 iteration: 14.9951276779\n",
      "Loss after 5100 iteration: 5.24673032761\n",
      "Loss after 5200 iteration: 2.15450882912\n",
      "Loss after 5300 iteration: 4.25470590591\n",
      "Loss after 5400 iteration: 1.00080144405\n",
      "Loss after 5500 iteration: 0.531034171581\n",
      "Loss after 5600 iteration: 0.508361876011\n",
      "Loss after 5700 iteration: 0.406906753778\n",
      "Loss after 5800 iteration: 4.29947900772\n",
      "Loss after 5900 iteration: 0.612687647343\n",
      "Loss after 6000 iteration: 0.0915969014168\n",
      "Loss after 6100 iteration: 0.459852457047\n",
      "Loss after 6200 iteration: 0.493513613939\n",
      "Loss after 6300 iteration: 0.403164893389\n",
      "Loss after 6400 iteration: 0.750332355499\n",
      "Loss after 6500 iteration: 1.86835753918\n",
      "Loss after 6600 iteration: 1.06126248837\n",
      "Loss after 6700 iteration: 0.603210628033\n",
      "Loss after 6800 iteration: 0.354505777359\n",
      "Loss after 6900 iteration: 1.43293631077\n",
      "Loss after 7000 iteration: 0.778225421906\n",
      "Loss after 7100 iteration: 1.05034291744\n",
      "Loss after 7200 iteration: 0.195344522595\n",
      "Loss after 7300 iteration: 1.28061103821\n",
      "Loss after 7400 iteration: 0.169609606266\n",
      "Loss after 7500 iteration: 0.383916288614\n",
      "Loss after 7600 iteration: 0.390112876892\n",
      "Loss after 7700 iteration: 0.226396486163\n",
      "Loss after 7800 iteration: 0.670483767986\n",
      "Loss after 7900 iteration: 0.113394260406\n",
      "Loss after 8000 iteration: 48.2177429199\n",
      "Loss after 8100 iteration: 36.0228881836\n",
      "Loss after 8200 iteration: 2.39915847778\n",
      "Loss after 8300 iteration: 32.4965362549\n",
      "Loss after 8400 iteration: 0.691551208496\n",
      "Loss after 8500 iteration: 7.44204282761\n",
      "Loss after 8600 iteration: 0.41908621788\n",
      "Loss after 8700 iteration: 5.44976758957\n",
      "Loss after 8800 iteration: 1.27061975002\n",
      "Loss after 8900 iteration: 0.700258433819\n",
      "Loss after 9000 iteration: 0.454001635313\n",
      "Loss after 9100 iteration: 0.244554102421\n",
      "Loss after 9200 iteration: 0.340216159821\n",
      "Loss after 9300 iteration: 0.675595581532\n",
      "Loss after 9400 iteration: 0.265198916197\n",
      "Loss after 9500 iteration: 0.260598629713\n",
      "Loss after 9600 iteration: 0.703088581562\n",
      "Loss after 9700 iteration: 0.168938204646\n",
      "Loss after 9800 iteration: 2.56613278389\n",
      "Loss after 9900 iteration: 1.05714547634\n",
      "Loss after 10000 iteration: 1.1287561655\n",
      "Loss after 10100 iteration: 0.172195985913\n",
      "Loss after 10200 iteration: 1.02822744846\n",
      "Loss after 10300 iteration: 0.354170829058\n",
      "Loss after 10400 iteration: 0.232679829001\n",
      "Loss after 10500 iteration: 0.440833061934\n",
      "Loss after 10600 iteration: 2.45590424538\n",
      "Loss after 10700 iteration: 0.540488421917\n",
      "Loss after 10800 iteration: 0.514607608318\n",
      "Loss after 10900 iteration: 0.490308910608\n",
      "Loss after 11000 iteration: 0.37231361866\n",
      "Loss after 11100 iteration: 0.303673058748\n",
      "Loss after 11200 iteration: 0.197373077273\n",
      "Loss after 11300 iteration: 0.590646922588\n",
      "Loss after 11400 iteration: 0.142871677876\n",
      "Loss after 11500 iteration: 0.163373842835\n",
      "Loss after 11600 iteration: 0.230375826359\n",
      "Loss after 11700 iteration: 1.06222975254\n",
      "Loss after 11800 iteration: 0.138787552714\n",
      "Loss after 11900 iteration: 0.157539203763\n",
      "Loss after 12000 iteration: 0.161739245057\n",
      "Loss after 12100 iteration: 0.517128050327\n",
      "Loss after 12200 iteration: 0.174005582929\n",
      "Loss after 12300 iteration: 0.187931790948\n",
      "Loss after 12400 iteration: 0.385377794504\n",
      "Loss after 12500 iteration: 0.289363950491\n",
      "Loss after 12600 iteration: 0.689595460892\n",
      "Loss after 12700 iteration: 0.401888877153\n",
      "Loss after 12800 iteration: 0.10549274832\n",
      "Loss after 12900 iteration: 0.160548254848\n",
      "Loss after 13000 iteration: 0.230742499232\n",
      "Loss after 13100 iteration: 0.383416503668\n",
      "Loss after 13200 iteration: 0.103695295751\n",
      "Loss after 13300 iteration: 0.345997422934\n",
      "Loss after 13400 iteration: 0.219658479095\n",
      "Loss after 13500 iteration: 0.184469625354\n",
      "Loss after 13600 iteration: 0.153270468116\n",
      "Loss after 13700 iteration: 0.263393551111\n",
      "Loss after 13800 iteration: 0.0731543973088\n",
      "Loss after 13900 iteration: 94237.0078125\n",
      "Loss after 14000 iteration: 138887.109375\n",
      "Loss after 14100 iteration: 109626.585938\n",
      "Loss after 14200 iteration: 72774.5703125\n",
      "Loss after 14300 iteration: 82386.2109375\n",
      "Loss after 14400 iteration: 58194.0625\n",
      "Loss after 14500 iteration: 2.14836764336\n",
      "Loss after 14600 iteration: 22911.7636719\n",
      "Loss after 14700 iteration: 22179.2207031\n",
      "Loss after 14800 iteration: 22736.9472656\n",
      "Loss after 14900 iteration: 0.880805075169\n",
      "Loss after 15000 iteration: 11316.0322266\n",
      "Loss after 15100 iteration: 0.656321108341\n",
      "Loss after 15200 iteration: 1.02620208263\n",
      "Loss after 15300 iteration: 1.84289073944\n",
      "Loss after 15400 iteration: 2.66841697693\n",
      "Loss after 15500 iteration: 0.958842515945\n",
      "Loss after 15600 iteration: 0.363266557455\n",
      "Loss after 15700 iteration: 11231.6474609\n",
      "Loss after 15800 iteration: 0.530038654804\n",
      "Loss after 15900 iteration: 0.754970133305\n",
      "Loss after 16000 iteration: 55180.625\n",
      "Loss after 16100 iteration: 147468.609375\n",
      "Loss after 16200 iteration: 80889.90625\n",
      "Loss after 16300 iteration: 50032.3710938\n",
      "Loss after 16400 iteration: 48793.0742188\n",
      "Loss after 16500 iteration: 38977.4882812\n",
      "Loss after 16600 iteration: 19274.5683594\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[666] = [22,2500] is out of bounds: need 0 <= index < [96,2500]\n\t [[Node: SparseToDense = SparseToDense[T=DT_FLOAT, Tindices=DT_INT64, validate_indices=true](SparseSlice, SparseSlice:2, SparseSlice:1, SparseToDense/default_value)]]\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,99], [?,?]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-220-eab58fa9a73d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0mn_iteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/maxime/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/maxime/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/maxime/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/maxime/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[666] = [22,2500] is out of bounds: need 0 <= index < [96,2500]\n\t [[Node: SparseToDense = SparseToDense[T=DT_FLOAT, Tindices=DT_INT64, validate_indices=true](SparseSlice, SparseSlice:2, SparseSlice:1, SparseToDense/default_value)]]\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,99], [?,?]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        sess.run(iterator.initializer)\n",
    "        current_loss = sess.run(loss)\n",
    "        n_iteration = 0\n",
    "        print(\"Loss after {} epochs: {}\".format(epoch,current_loss))\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                _, current_loss = sess.run([training_op,loss])\n",
    "                n_iteration += 1\n",
    "                \n",
    "                if not(n_iteration%100):\n",
    "                    print(\"Loss after {} iteration: {}\".format(n_iteration,current_loss)) \n",
    "            except tf.errors.OutOfRangeError:\n",
    "                saver.save(sess, \"/ModelWeights/test_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat = loadmat(train_addrs[0])\n",
    "features = list(mat['structarr'][0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_raster_base = features[28].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spike_raster_base"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
